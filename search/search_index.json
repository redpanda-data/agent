{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Redpanda Agents","text":"<p>The Redpanda Agent SDK allows you to build agentic Redpanda Connect pipelines.</p> <p>You can use <code>rpk</code> to generate the initial boilerplate for an agentic pipeline.</p> <pre><code>$ rpk connect agent init my_first_agent\n\n$ ls --tree ./my_first_agent\nmy_first_agent\n\u251c\u2500\u2500 agents\n\u2502   \u2514\u2500\u2500 weather.py\n\u251c\u2500\u2500 mcp\n\u2502   \u2514\u2500\u2500 resources\n\u2502       \u2514\u2500\u2500 processors\n\u2502           \u2514\u2500\u2500 check_weather_tool.yaml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 redpanda_agents.yaml\n\u2514\u2500\u2500 uv.lock\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<p>The project structure is as follows:</p>"},{"location":"#redpanda_agentsyaml","title":"<code>redpanda_agents.yaml</code>","text":"<p>The main entry point for the agent pipeline. The file looks like the following:</p> <pre><code># Each agent is an entry under `agents` - there can be multiple for multi-agent flows.\nagents:\n  # The name of the agent determines what python file is executed for the agent.\n  &lt;agent_name&gt;:\n    # Any Redpanda Connect input is valid here\n    # See them all at: https://docs.redpanda.com/redpanda-connect/components/inputs/about/\n    input:\n      &lt;input&gt;\n    # Any tool labels defined in the `mcp` directory, see notes below for more.\n    tools:\n      - &lt;tool label&gt;\n    # Any Redpanda Connect output is valid here\n    # See them all at https://docs.redpanda.com/redpanda-connect/components/outputs/about/\n    output:\n      &lt;output&gt;\n    # Configure Redpanda Connect to send tracing events to Jaeger, Open Telemetry collector and more.\n    # See options here: https://docs.redpanda.com/redpanda-connect/components/tracers/about/\n    tracer:\n        &lt;tracer&gt;\n</code></pre>"},{"location":"#agentspy","title":"<code>agents/*.py</code>","text":"<p>Each agent receives input from <code>input</code> and sends its output to <code>output</code>. You can create an agent by importing from <code>redpanda.agents</code>. Creating an <code>Agent</code> looks like:</p> <pre><code>my_agent = Agent(\n    name=\"my_first_agent\",\n    model=\"openai/gpt-4o\",\n    instructions=\"These are your instructions - good luck!\",\n)\n</code></pre> <p>In this example, OpenAI GPT-4o is configured and requires you to set <code>OPENAI_API_KEY</code> as an environment variable.</p> <p>Once you've created the agent, you pass it off to the runtime to handle messages in the pipeline like so:</p> <pre><code>asyncio.run(redpanda.runtime.serve(my_agent))\n</code></pre>"},{"location":"#mcpresourcesprocessorsyaml","title":"<code>mcp/resources/processors/*.yaml</code>","text":"<p>In order to give tools to your agent, you need to first define them as yaml files with the following structure:</p> <pre><code>label: '&lt;label&gt;'\nprocessors:\n  - &lt;processors&gt;\nmeta:\n  mcp:\n    enabled: true\n    description: '&lt;description&gt;'\n</code></pre> <p>The <code>&lt;label&gt;</code> is what must be provided under the list of <code>tools</code> in the agent YAML file, while the processors can be any Redpanda Connect processor.</p>"},{"location":"#running","title":"Running","text":"<p>To run our agent, we can do that with <code>rpk connect agent run my_first_agent</code></p>"},{"location":"reference/agents/","title":"Agents module","text":""},{"location":"reference/agents/#redpanda.agents.Agent","title":"Agent","text":"<p>An agent is a wrapper around a LLM model that can generate responses to input text.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the agent.</p> <code>model</code> <code>str</code> <p>The LLM model that the agent will use. Models follow the format of \"/\" and are loaded using litellm. Examples:     \"openai/gpt-4o\"     \"gemini/gemini-pro\"     \"bedrock/bedrock-claude-v1\" <code>response_format</code> <code>type[BaseModel] | None</code> <p>The Pydantic model that the response will be validated against.</p> <code>parameters</code> <code>dict[str, Any]</code> <p>A dictionary of parameters that the model will use. These parameters are specific to the model. Examples:     {\"temperature\": 0.5}     {\"max_tokens\": 100}</p> <code>tools</code> <code>list[Tool]</code> <p>The tools exposed to the agent.</p> <code>mcp</code> <code>list[MCPEndpoint]</code> <p>The MCP endpoints that the agent can invoke.</p> <code>hooks</code> <code>AgentHooks</code> <p>Callbacks to invoke during various points of the agent runtime.</p> Source code in <code>src/redpanda/agents/_agent.py</code> <pre><code>class Agent:\n    \"\"\"\n    An agent is a wrapper around a LLM model that can generate responses to input text.\n\n    Attributes:\n        name: The name of the agent.\n        model: The LLM model that the agent will use. Models follow the format of\n            \"&lt;provider&gt;/&lt;model&gt;\" and are loaded using [litellm](https://docs.litellm.ai/docs/providers).\n            Examples:\n                \"openai/gpt-4o\"\n                \"gemini/gemini-pro\"\n                \"bedrock/bedrock-claude-v1\"\n        response_format: The Pydantic model that the response will be validated against.\n        parameters: A dictionary of parameters that the model will use.\n            These parameters are specific to the model.\n            Examples:\n                {\"temperature\": 0.5}\n                {\"max_tokens\": 100}\n                {\"temperature\": 0.5, \"max_tokens\": 100}\n        tools: The tools exposed to the agent.\n        mcp: The MCP endpoints that the agent can invoke.\n        hooks: Callbacks to invoke during various points of the agent runtime.\n    \"\"\"\n\n    name: str\n    model: str\n    instructions: str | None\n    response_format: type[BaseModel] | None\n    parameters: dict[str, Any]\n    tools: list[Tool]\n    mcp: list[MCPEndpoint]\n    hooks: AgentHooks\n\n    def __init__(\n        self,\n        name: str,\n        model: str,\n        instructions: str | None = None,\n        response_type: type[BaseModel] | None = None,\n        tools: list[Tool] | None = None,\n        mcp: list[MCPEndpoint] | None = None,\n        hooks: AgentHooks | None = None,\n        **kwargs: Any,\n    ):\n        \"\"\"\n        Args:\n            name: The name of the agent.\n            model: The LLM model that the agent will use. Models follow the format of\n                \"&lt;provider&gt;/&lt;model&gt;\" and are loaded using [litellm](https://docs.litellm.ai/docs/providers).\n                Examples:\n                    \"openai/gpt-4o\"\n                    \"gemini/gemini-pro\"\n                    \"bedrock/bedrock-claude-v1\"\n            instructions: The system prompt for the agent.\n            response_type: The Pydantic model that the response will be validated against.\n                If None, the response will be a string.\n            tools: The tools exposed to the agent.\n            mcp: The MCP endpoints that the agent can invoke.\n            hooks: Callbacks to invoke during various points of the agent runtime.\n            **kwargs: A dictionary of parameters that the model will use.\n                These parameters are specific to the model.\n                Examples:\n                    {\"temperature\": 0.5}\n                    {\"max_tokens\": 100}\n                    {\"temperature\": 0.5, \"max_tokens\": 100}\n        \"\"\"\n        self.name = name\n        self.model = model\n        self.instructions = instructions\n        self.response_format = response_type\n        self.parameters = kwargs\n        self.tools = tools or []\n        self.mcp = mcp or []\n        self.hooks = hooks or AgentHooks()\n\n    async def run(self, input: str) -&gt; Any:\n        \"\"\"\n        Generate a response from the model given an input text.\n\n        Args:\n            input: The input text that the model will use to generate a response.\n        Returns:\n            The generated response from the model.\n        \"\"\"\n        await self.hooks.on_start(self)\n        async with AsyncExitStack() as stack:\n            mcp_clients: list[MCPClient] = []\n            for server in self.mcp:\n                mcp_clients.append(await stack.enter_async_context(mcp_client(server)))\n\n            tools = {tool.name: tool for tool in self.tools}\n\n            for client in mcp_clients:\n                await client.initialize()\n                for tool in await client.list_tools():\n                    if tool.name not in tools:\n                        tools[tool.name] = tool\n                    else:\n                        # TODO: Warn on conflicting tools?\n                        pass\n\n            tool_defs = [\n                {\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": tool.name,\n                        \"description\": tool.description,\n                        \"parameters\": tool.parameters,\n                    },\n                }\n                for tool in tools.values()\n            ]\n            messages: list[dict[str, str] | Message] = [{\"role\": \"user\", \"content\": input}]\n            if self.instructions:\n                messages = [{\"role\": \"system\", \"content\": self.instructions}] + messages\n            while True:\n                model_resp = await self._invoke_llm(messages, tool_defs)\n                choice_resp = model_resp.choices[-1]\n                if isinstance(choice_resp, StreamingChoices):\n                    raise Exception(\"unexpected streaming response type\")\n                if choice_resp.message.tool_calls:\n                    messages.append(choice_resp.message)\n                    messages.extend(await self._call_tools(choice_resp.message.tool_calls, tools))\n                    continue\n                output = choice_resp.message.content\n                if output is None:\n                    raise Exception(\"unexpected response type of None\")\n                if self.response_format is not None:\n                    output = self.response_format.model_validate_json(output)\n                await self.hooks.on_end(self, output)\n                return output\n\n    async def _invoke_llm(\n        self, messages: list[dict[str, str] | Message], tool_defs: list[dict[str, Any]]\n    ):\n        with trace.get_tracer(\"redpanda.agent\").start_as_current_span(\"invoke_llm\") as span:\n            span.set_attribute(\"model\", self.model)\n            span.set_attribute(\n                \"messages\",\n                json.dumps([m.model_dump() if isinstance(m, BaseModel) else m for m in messages]),\n            )\n            resp = await acompletion(\n                model=self.model,\n                response_format=self.response_format,\n                messages=messages,\n                tools=tool_defs,\n                **self.parameters,\n            )\n            if isinstance(resp, CustomStreamWrapper):\n                raise Exception(\"unexpected response type of CustomStreamWrapper\")\n            if hasattr(resp, \"usage\") and isinstance(resp.usage, Usage):  # pyright: ignore[reportAttributeAccessIssue,reportUnknownMemberType]\n                usage = resp.usage  # pyright: ignore[reportAttributeAccessIssue]\n                span.set_attribute(\"completion_tokens\", usage.completion_tokens)\n                span.set_attribute(\"prompt_tokens\", usage.prompt_tokens)\n                span.set_attribute(\"total_tokens\", usage.total_tokens)\n                if usage.completion_tokens_details:\n                    ctd = usage.completion_tokens_details\n                    if ctd.accepted_prediction_tokens:\n                        span.set_attribute(\n                            \"accepted_prediction_tokens\", ctd.accepted_prediction_tokens\n                        )\n                    if ctd.rejected_prediction_tokens:\n                        span.set_attribute(\n                            \"rejected_reasoning_tokens\", ctd.rejected_prediction_tokens\n                        )\n                    if ctd.reasoning_tokens:\n                        span.set_attribute(\"reasoning_tokens\", ctd.reasoning_tokens)\n                if usage.prompt_tokens_details:\n                    ptd = usage.prompt_tokens_details\n                    if ptd.cached_tokens:\n                        span.set_attribute(\"cached_tokens\", ptd.cached_tokens)\n            span.set_attribute(\"response\", json.dumps([m.model_dump() for m in resp.choices]))\n            return resp\n\n    async def _call_tools(\n        self, tool_calls: list[ChatCompletionMessageToolCall], tools: dict[str, Tool]\n    ) -&gt; list[dict[str, Any]]:\n        messages: list[dict[str, Any]] = []\n        for tool_call in tool_calls:\n            func = tool_call.function\n            selected = func.name and tools.get(func.name)\n            if not selected:\n                raise Exception(f\"tool {func.name} not found\")\n            await self.hooks.on_tool_start(self, selected, func.arguments)\n            resp = await selected(json.loads(func.arguments))\n            if isinstance(resp, ToolResponse):\n                output: Any = []\n                for content in resp.content:\n                    if content.type == \"text\":\n                        output.append(\n                            {\n                                \"type\": \"text\",\n                                \"text\": content.data,\n                            }\n                        )\n                    elif content.type == \"image\":\n                        output.append(\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:{content.mime_type};base64,{content.data}\",\n                                },\n                            }\n                        )\n                    else:\n                        raise NotImplementedError(f\"Unknown content type: {content.type}\")\n            if isinstance(resp, BaseModel):\n                output = resp.model_dump_json()\n            else:\n                output = json.dumps(resp)\n            await self.hooks.on_tool_end(\n                self, selected, output if isinstance(output, str) else json.dumps(output)\n            )\n            messages.append(\n                {\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": selected.name,\n                    \"content\": output,\n                }\n            )\n        return messages\n\n    def as_tool(self) -&gt; Tool:\n        # TODO(rockwood): support handoffs and passing more context\n        return AgentTool(self)\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.Agent.__init__","title":"__init__","text":"<pre><code>__init__(\n    name: str,\n    model: str,\n    instructions: str | None = None,\n    response_type: type[BaseModel] | None = None,\n    tools: list[Tool] | None = None,\n    mcp: list[MCPEndpoint] | None = None,\n    hooks: AgentHooks | None = None,\n    **kwargs: Any,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the agent.</p> required <code>model</code> <code>str</code> <p>The LLM model that the agent will use. Models follow the format of \"/\" and are loaded using litellm. Examples:     \"openai/gpt-4o\"     \"gemini/gemini-pro\"     \"bedrock/bedrock-claude-v1\" required <code>instructions</code> <code>str | None</code> <p>The system prompt for the agent.</p> <code>None</code> <code>response_type</code> <code>type[BaseModel] | None</code> <p>The Pydantic model that the response will be validated against. If None, the response will be a string.</p> <code>None</code> <code>tools</code> <code>list[Tool] | None</code> <p>The tools exposed to the agent.</p> <code>None</code> <code>mcp</code> <code>list[MCPEndpoint] | None</code> <p>The MCP endpoints that the agent can invoke.</p> <code>None</code> <code>hooks</code> <code>AgentHooks | None</code> <p>Callbacks to invoke during various points of the agent runtime.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>A dictionary of parameters that the model will use. These parameters are specific to the model. Examples:     {\"temperature\": 0.5}     {\"max_tokens\": 100}</p> <code>{}</code> Source code in <code>src/redpanda/agents/_agent.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    model: str,\n    instructions: str | None = None,\n    response_type: type[BaseModel] | None = None,\n    tools: list[Tool] | None = None,\n    mcp: list[MCPEndpoint] | None = None,\n    hooks: AgentHooks | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Args:\n        name: The name of the agent.\n        model: The LLM model that the agent will use. Models follow the format of\n            \"&lt;provider&gt;/&lt;model&gt;\" and are loaded using [litellm](https://docs.litellm.ai/docs/providers).\n            Examples:\n                \"openai/gpt-4o\"\n                \"gemini/gemini-pro\"\n                \"bedrock/bedrock-claude-v1\"\n        instructions: The system prompt for the agent.\n        response_type: The Pydantic model that the response will be validated against.\n            If None, the response will be a string.\n        tools: The tools exposed to the agent.\n        mcp: The MCP endpoints that the agent can invoke.\n        hooks: Callbacks to invoke during various points of the agent runtime.\n        **kwargs: A dictionary of parameters that the model will use.\n            These parameters are specific to the model.\n            Examples:\n                {\"temperature\": 0.5}\n                {\"max_tokens\": 100}\n                {\"temperature\": 0.5, \"max_tokens\": 100}\n    \"\"\"\n    self.name = name\n    self.model = model\n    self.instructions = instructions\n    self.response_format = response_type\n    self.parameters = kwargs\n    self.tools = tools or []\n    self.mcp = mcp or []\n    self.hooks = hooks or AgentHooks()\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.Agent.run","title":"run  <code>async</code>","text":"<pre><code>run(input: str) -&gt; Any\n</code></pre> <p>Generate a response from the model given an input text.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The input text that the model will use to generate a response.</p> required <p>Returns:     The generated response from the model.</p> Source code in <code>src/redpanda/agents/_agent.py</code> <pre><code>async def run(self, input: str) -&gt; Any:\n    \"\"\"\n    Generate a response from the model given an input text.\n\n    Args:\n        input: The input text that the model will use to generate a response.\n    Returns:\n        The generated response from the model.\n    \"\"\"\n    await self.hooks.on_start(self)\n    async with AsyncExitStack() as stack:\n        mcp_clients: list[MCPClient] = []\n        for server in self.mcp:\n            mcp_clients.append(await stack.enter_async_context(mcp_client(server)))\n\n        tools = {tool.name: tool for tool in self.tools}\n\n        for client in mcp_clients:\n            await client.initialize()\n            for tool in await client.list_tools():\n                if tool.name not in tools:\n                    tools[tool.name] = tool\n                else:\n                    # TODO: Warn on conflicting tools?\n                    pass\n\n        tool_defs = [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.parameters,\n                },\n            }\n            for tool in tools.values()\n        ]\n        messages: list[dict[str, str] | Message] = [{\"role\": \"user\", \"content\": input}]\n        if self.instructions:\n            messages = [{\"role\": \"system\", \"content\": self.instructions}] + messages\n        while True:\n            model_resp = await self._invoke_llm(messages, tool_defs)\n            choice_resp = model_resp.choices[-1]\n            if isinstance(choice_resp, StreamingChoices):\n                raise Exception(\"unexpected streaming response type\")\n            if choice_resp.message.tool_calls:\n                messages.append(choice_resp.message)\n                messages.extend(await self._call_tools(choice_resp.message.tool_calls, tools))\n                continue\n            output = choice_resp.message.content\n            if output is None:\n                raise Exception(\"unexpected response type of None\")\n            if self.response_format is not None:\n                output = self.response_format.model_validate_json(output)\n            await self.hooks.on_end(self, output)\n            return output\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.AgentHooks","title":"AgentHooks","text":"<p>A class that receives callbacks on various lifecycle events for a specific agent.</p> Source code in <code>src/redpanda/agents/_agent.py</code> <pre><code>class AgentHooks:\n    \"\"\"\n    A class that receives callbacks on various lifecycle events for a specific agent.\n    \"\"\"\n\n    async def on_start(\n        self,\n        agent: \"Agent\",\n    ) -&gt; None:\n        \"\"\"Called before the agent is invoked.\"\"\"\n        _ = agent\n\n    async def on_end(\n        self,\n        agent: \"Agent\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Called when the agent produces a final output.\"\"\"\n        _ = agent, output\n\n    async def on_tool_start(\n        self,\n        agent: \"Agent\",\n        tool: Tool,\n        args: str,\n    ) -&gt; None:\n        \"\"\"Called before a tool is invoked.\"\"\"\n        _ = agent, tool, args\n\n    async def on_tool_end(\n        self,\n        agent: \"Agent\",\n        tool: Tool,\n        result: str,\n    ) -&gt; None:\n        \"\"\"Called after a tool is invoked.\"\"\"\n        _ = agent, tool, result\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.AgentHooks.on_start","title":"on_start  <code>async</code>","text":"<pre><code>on_start(agent: Agent) -&gt; None\n</code></pre> <p>Called before the agent is invoked.</p> Source code in <code>src/redpanda/agents/_agent.py</code> <pre><code>async def on_start(\n    self,\n    agent: \"Agent\",\n) -&gt; None:\n    \"\"\"Called before the agent is invoked.\"\"\"\n    _ = agent\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.AgentHooks.on_end","title":"on_end  <code>async</code>","text":"<pre><code>on_end(agent: Agent, output: Any) -&gt; None\n</code></pre> <p>Called when the agent produces a final output.</p> Source code in <code>src/redpanda/agents/_agent.py</code> <pre><code>async def on_end(\n    self,\n    agent: \"Agent\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Called when the agent produces a final output.\"\"\"\n    _ = agent, output\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.AgentHooks.on_tool_start","title":"on_tool_start  <code>async</code>","text":"<pre><code>on_tool_start(agent: Agent, tool: Tool, args: str) -&gt; None\n</code></pre> <p>Called before a tool is invoked.</p> Source code in <code>src/redpanda/agents/_agent.py</code> <pre><code>async def on_tool_start(\n    self,\n    agent: \"Agent\",\n    tool: Tool,\n    args: str,\n) -&gt; None:\n    \"\"\"Called before a tool is invoked.\"\"\"\n    _ = agent, tool, args\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.AgentHooks.on_tool_end","title":"on_tool_end  <code>async</code>","text":"<pre><code>on_tool_end(agent: Agent, tool: Tool, result: str) -&gt; None\n</code></pre> <p>Called after a tool is invoked.</p> Source code in <code>src/redpanda/agents/_agent.py</code> <pre><code>async def on_tool_end(\n    self,\n    agent: \"Agent\",\n    tool: Tool,\n    result: str,\n) -&gt; None:\n    \"\"\"Called after a tool is invoked.\"\"\"\n    _ = agent, tool, result\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.MCPEndpoint","title":"MCPEndpoint","text":"<p>An Base class for all endpoints (ways to connect) to MCP servers.</p> Source code in <code>src/redpanda/agents/_mcp.py</code> <pre><code>class MCPEndpoint:\n    \"\"\"\n    An Base class for all endpoints (ways to connect) to MCP servers.\n    \"\"\"\n\n    # TODO(rockwood): support list change notifications\n    _cache_enabled: bool\n    _cached_tool_list: list[MCPToolDef] | None = None\n\n    def __init__(self, cache_enabled: bool) -&gt; None:\n        self._cache_enabled = cache_enabled\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.SSEMCPEndpoint","title":"SSEMCPEndpoint","text":"<p>               Bases: <code>MCPEndpoint</code></p> <p>A MCP endpoint that communicates with an MCP server over Server-Sent Events.</p> Source code in <code>src/redpanda/agents/_mcp.py</code> <pre><code>class SSEMCPEndpoint(MCPEndpoint):\n    \"\"\"\n    A MCP endpoint that communicates with an MCP server over Server-Sent Events.\n    \"\"\"\n\n    url: str\n\n    def __init__(self, url: str, cache_enabled: bool = True):\n        \"\"\"\n        Create a new SSEMCPEndpoint instance.\n\n        Args:\n            url: The URL of the SSE server.\n            cache_enabled: Whether to cache the list of {tools,resources,prompts} from the server.\n        \"\"\"\n        super().__init__(cache_enabled)\n        self.url = url\n\n    @property\n    def headers(self) -&gt; dict[str, Any]:\n        return {}\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.SSEMCPEndpoint.__init__","title":"__init__","text":"<pre><code>__init__(url: str, cache_enabled: bool = True)\n</code></pre> <p>Create a new SSEMCPEndpoint instance.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the SSE server.</p> required <code>cache_enabled</code> <code>bool</code> <p>Whether to cache the list of {tools,resources,prompts} from the server.</p> <code>True</code> Source code in <code>src/redpanda/agents/_mcp.py</code> <pre><code>def __init__(self, url: str, cache_enabled: bool = True):\n    \"\"\"\n    Create a new SSEMCPEndpoint instance.\n\n    Args:\n        url: The URL of the SSE server.\n        cache_enabled: Whether to cache the list of {tools,resources,prompts} from the server.\n    \"\"\"\n    super().__init__(cache_enabled)\n    self.url = url\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.StdioMCPEndpoint","title":"StdioMCPEndpoint","text":"<p>               Bases: <code>MCPEndpoint</code></p> <p>A MCP endpoint that invokes a local process and communicates over stdin/stdout.</p> Source code in <code>src/redpanda/agents/_mcp.py</code> <pre><code>class StdioMCPEndpoint(MCPEndpoint):\n    \"\"\"\n    A MCP endpoint that invokes a local process and communicates over stdin/stdout.\n    \"\"\"\n\n    params: StdioServerParameters\n\n    def __init__(self, params: StdioServerParameters, cache_enabled: bool = True):\n        \"\"\"\n        Create a new StdioMCPEndpoint instance.\n\n        Args:\n            params: The parameters for the server.\n            cache_enabled: Whether to cache the list of {tools,resources,prompts} from the server.\n        \"\"\"\n        super().__init__(cache_enabled)\n        self.params = params\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.StdioMCPEndpoint.__init__","title":"__init__","text":"<pre><code>__init__(\n    params: StdioServerParameters,\n    cache_enabled: bool = True,\n)\n</code></pre> <p>Create a new StdioMCPEndpoint instance.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>StdioServerParameters</code> <p>The parameters for the server.</p> required <code>cache_enabled</code> <code>bool</code> <p>Whether to cache the list of {tools,resources,prompts} from the server.</p> <code>True</code> Source code in <code>src/redpanda/agents/_mcp.py</code> <pre><code>def __init__(self, params: StdioServerParameters, cache_enabled: bool = True):\n    \"\"\"\n    Create a new StdioMCPEndpoint instance.\n\n    Args:\n        params: The parameters for the server.\n        cache_enabled: Whether to cache the list of {tools,resources,prompts} from the server.\n    \"\"\"\n    super().__init__(cache_enabled)\n    self.params = params\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.StreamableHTTPMCPEndpoint","title":"StreamableHTTPMCPEndpoint","text":"<p>               Bases: <code>MCPEndpoint</code></p> <p>A MCP endpoint that communicates with an MCP server over HTTP streaming.</p> Source code in <code>src/redpanda/agents/_mcp.py</code> <pre><code>class StreamableHTTPMCPEndpoint(MCPEndpoint):\n    \"\"\"\n    A MCP endpoint that communicates with an MCP server over HTTP streaming.\n    \"\"\"\n\n    url: str\n\n    def __init__(self, url: str, cache_enabled: bool = True):\n        \"\"\"\n        Create a new StreamableHTTPMCPEndpoint instance.\n\n        Args:\n            url: The URL of the HTTP server.\n            cache_enabled: Whether to cache the list of {tools,resources,prompts} from the server.\n        \"\"\"\n        super().__init__(cache_enabled)\n        self.url = url\n\n    @property\n    def headers(self) -&gt; dict[str, Any]:\n        return {}\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.StreamableHTTPMCPEndpoint.__init__","title":"__init__","text":"<pre><code>__init__(url: str, cache_enabled: bool = True)\n</code></pre> <p>Create a new StreamableHTTPMCPEndpoint instance.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the HTTP server.</p> required <code>cache_enabled</code> <code>bool</code> <p>Whether to cache the list of {tools,resources,prompts} from the server.</p> <code>True</code> Source code in <code>src/redpanda/agents/_mcp.py</code> <pre><code>def __init__(self, url: str, cache_enabled: bool = True):\n    \"\"\"\n    Create a new StreamableHTTPMCPEndpoint instance.\n\n    Args:\n        url: The URL of the HTTP server.\n        cache_enabled: Whether to cache the list of {tools,resources,prompts} from the server.\n    \"\"\"\n    super().__init__(cache_enabled)\n    self.url = url\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.WebsocketMCPEndpoint","title":"WebsocketMCPEndpoint","text":"<p>               Bases: <code>MCPEndpoint</code></p> <p>A MCP endpoint that communicates with an MCP server over a WebSocket.</p> Source code in <code>src/redpanda/agents/_mcp.py</code> <pre><code>class WebsocketMCPEndpoint(MCPEndpoint):\n    \"\"\"\n    A MCP endpoint that communicates with an MCP server over a WebSocket.\n    \"\"\"\n\n    url: str\n\n    def __init__(self, url: str, cache_enabled: bool = True):\n        \"\"\"\n        Create a new WebsocketMCPEndpoint instance.\n\n        Args:\n            url: The URL of the WebSocket server.\n            cache_enabled: Whether to cache the list of {tools,resources,prompts} from the server.\n        \"\"\"\n        super().__init__(cache_enabled)\n        self.url = url\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.WebsocketMCPEndpoint.__init__","title":"__init__","text":"<pre><code>__init__(url: str, cache_enabled: bool = True)\n</code></pre> <p>Create a new WebsocketMCPEndpoint instance.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the WebSocket server.</p> required <code>cache_enabled</code> <code>bool</code> <p>Whether to cache the list of {tools,resources,prompts} from the server.</p> <code>True</code> Source code in <code>src/redpanda/agents/_mcp.py</code> <pre><code>def __init__(self, url: str, cache_enabled: bool = True):\n    \"\"\"\n    Create a new WebsocketMCPEndpoint instance.\n\n    Args:\n        url: The URL of the WebSocket server.\n        cache_enabled: Whether to cache the list of {tools,resources,prompts} from the server.\n    \"\"\"\n    super().__init__(cache_enabled)\n    self.url = url\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.Tool","title":"Tool","text":"<p>A tool is a function that can be called by an LLM with a set of parameters.</p> Source code in <code>src/redpanda/agents/_tools.py</code> <pre><code>class Tool:\n    \"\"\"\n    A tool is a function that can be called by an LLM with a set of parameters.\n    \"\"\"\n\n    name: str\n    \"\"\"\n    The name of the tool.\n    \"\"\"\n    description: str | None\n    \"\"\"\n    An optional description of the tool.\n    \"\"\"\n    parameters: dict[str, Any]\n    \"\"\"\n    A dictionary of parameters (json_schema) that the tool requires.\n    \"\"\"\n\n    def __init__(self, name: str, description: str | None, parameters: dict[str, Any]):\n        \"\"\"\n        Initialize a new tool.\n        \"\"\"\n        self.name = name\n        self.description = description\n        self.parameters = parameters\n\n    async def __call__(self, args: dict[str, Any]) -&gt; Any:\n        \"\"\"\n        Call the tool with the given arguments (should match the provided schema).\n\n        The return result can be:\n        - Pydantic model, which will be serialized to JSON and passed back to the model as text.\n        - string, which will be passed back to the model as text.\n        - ToolResponse, which allows for more structured content to be passed back to the model.\n        - Anything else will be serialized using `json.dumps` and passed back to the model as text.\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.Tool.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str = name\n</code></pre> <p>The name of the tool.</p>"},{"location":"reference/agents/#redpanda.agents.Tool.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str | None = description\n</code></pre> <p>An optional description of the tool.</p>"},{"location":"reference/agents/#redpanda.agents.Tool.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters: dict[str, Any] = parameters\n</code></pre> <p>A dictionary of parameters (json_schema) that the tool requires.</p>"},{"location":"reference/agents/#redpanda.agents.Tool.__init__","title":"__init__","text":"<pre><code>__init__(\n    name: str,\n    description: str | None,\n    parameters: dict[str, Any],\n)\n</code></pre> <p>Initialize a new tool.</p> Source code in <code>src/redpanda/agents/_tools.py</code> <pre><code>def __init__(self, name: str, description: str | None, parameters: dict[str, Any]):\n    \"\"\"\n    Initialize a new tool.\n    \"\"\"\n    self.name = name\n    self.description = description\n    self.parameters = parameters\n</code></pre>"},{"location":"reference/agents/#redpanda.agents.Tool.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__(args: dict[str, Any]) -&gt; Any\n</code></pre> <p>Call the tool with the given arguments (should match the provided schema).</p> <p>The return result can be: - Pydantic model, which will be serialized to JSON and passed back to the model as text. - string, which will be passed back to the model as text. - ToolResponse, which allows for more structured content to be passed back to the model. - Anything else will be serialized using <code>json.dumps</code> and passed back to the model as text.</p> Source code in <code>src/redpanda/agents/_tools.py</code> <pre><code>async def __call__(self, args: dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Call the tool with the given arguments (should match the provided schema).\n\n    The return result can be:\n    - Pydantic model, which will be serialized to JSON and passed back to the model as text.\n    - string, which will be passed back to the model as text.\n    - ToolResponse, which allows for more structured content to be passed back to the model.\n    - Anything else will be serialized using `json.dumps` and passed back to the model as text.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/runtime/","title":"Runtime module","text":""},{"location":"reference/runtime/#redpanda.runtime.serve","title":"serve  <code>async</code>","text":"<pre><code>serve(agent: Agent) -&gt; None\n</code></pre> <p>Serve an agent as a Redpanda Connect processor plugin.</p> <p>This method runs for the entire lifetime of the server.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent to serve.</p> required Source code in <code>src/redpanda/runtime/__init__.py</code> <pre><code>async def serve(agent: Agent) -&gt; None:\n    \"\"\"\n    Serve an agent as a Redpanda Connect processor plugin.\n\n    This method runs for the entire lifetime of the server.\n\n    Args:\n        agent: The agent to serve.\n    \"\"\"\n    provider = tracesdk.TracerProvider()\n    trace.set_tracer_provider(provider)\n    provider.add_span_processor(SimpleSpanProcessor(PassthroughTraceCollector()))\n    addr = os.getenv(\"REDPANDA_CONNECT_AGENT_RUNTIME_MCP_SERVER\")\n    if addr:\n        agent.mcp.append(_TracingSSEMCPEndpoint(addr))\n    server = RuntimeServer(agent, trace.get_tracer(\"redpanda.runtime\"))\n    await serve_main(server)\n</code></pre>"},{"location":"tutorial/","title":"Overview","text":"<p>Redpanda Agents provides a flexible system where an Agent can answer questions and use specialized Tools to tackle tasks.  The Agent can also connect to external services through different MCPEndpoints, letting it integrate with remote or local resources. </p> <p>A RuntimeServer exposes the Agent over gRPC, so Redpanda Connect can interact with it.  The project includes hooks to track key events (like before or after a tool is called) and a ToolResponse structure for sending back text or images neatly. </p>"},{"location":"tutorial/#chapters","title":"Chapters","text":"<ol> <li>Agent </li> <li>Tool </li> <li>ToolResponse </li> <li>AgentHooks </li> <li>MCPEndpoint </li> <li>mcp_client </li> <li>RuntimeServer </li> </ol> <p>Generated by AI Codebase Knowledge Builder</p>"},{"location":"tutorial/01_agent_/","title":"Chapter 1: Agent","text":"<p>Welcome to our journey in building an intelligent, extensible system! In this first chapter, we introduce the core entity called an \u201cAgent.\u201d Think of the Agent as a skilled operator who receives your question (or prompt) and figures out the best way to solve it. Sometimes, it will directly respond using its built-in language model, and other times it may call specialized helpers (Tools) to do certain tasks. By the end of this chapter, you will understand what an Agent is, why it\u2019s useful, and how to get started with it in your own code.</p>"},{"location":"tutorial/01_agent_/#1-why-do-we-need-an-agent","title":"1. Why Do We Need an Agent?","text":"<p>Imagine you have a chatbot that can not only answer questions but also perform specific tasks like searching the web, generating images, or performing calculations. You could build all these features directly into one big system, but it quickly becomes complicated.</p> <p>The Agent simplifies everything by acting as a \u201ccontrol tower\u201d: 1. It takes your request. 2. Figures out if it can answer directly. 3. If not, it delegates tasks to any relevant tools. 4. It compiles the final answer back to you.</p>"},{"location":"tutorial/01_agent_/#2-a-very-simple-use-case","title":"2. A Very Simple Use Case","text":"<p>Let\u2019s start with an ultra-simple scenario: we just want the Agent to respond to user messages using a language model. We won\u2019t worry about Tools just yet (we\u2019ll see them in Tool).</p> <p>Below is a short snippet showing how we can instantiate and use an Agent:</p> <pre><code># In a file like main.py\nimport asyncio\nfrom redpanda.agents import Agent\n\n# Create an agent with minimal setup\nmy_agent = Agent(\n    name=\"MyFirstAgent\",\n    model=\"openai/gpt-3.5-turbo\",      # Use your favorite model\n    instructions=\"Please answer in a friendly style.\"\n)\n\n# Now let's ask something\nasync def run_agent():\n   response = asyncio.run(my_agent.run(\"Hello Agent, what's your favorite color?\"))\n   print(response)  # It will print a response based on the language model\n\nasyncio.run(run_agent())\n</code></pre> <p>Explanation of the snippet: - We import the <code>Agent</code> from the <code>redpanda.agents</code> module. - We instantiate it by giving it a name, specifying which language model to use, and optionally providing instructions (like a system prompt). - We call <code>run(...)</code> with a user message, and the Agent replies with the model\u2019s generated text.</p>"},{"location":"tutorial/01_agent_/#3-key-concepts-inside-the-agent","title":"3. Key Concepts Inside the Agent","text":"<ol> <li> <p>Model Selection:    Determines which language model the agent uses to generate text.    Example: \u201copenai/gpt-3.5-turbo.\u201d  </p> </li> <li> <p>Instructions:    Act like the \u201crole\u201d or system prompt. You can set the tone or style of the Agent\u2019s answers here.  </p> </li> <li> <p>Hooks (AgentHooks):    Optional callbacks to track events like \u201cagent started,\u201d \u201ctool started,\u201d etc. Beginners can skip this.  </p> </li> <li> <p>Tools (Tool):    Specialized tasks the Agent can call. For now, we keep it minimal, but Tools help you do powerful things (like image generation, data lookups, etc.) when the Agent decides it needs them.  </p> </li> </ol>"},{"location":"tutorial/01_agent_/#4-inside-the-agent-whats-really-happening","title":"4. Inside the Agent: What\u2019s Really Happening?","text":"<p>When you call <code>my_agent.run(\"...\")</code>, here\u2019s a simplified flow:</p> <pre><code>sequenceDiagram\n    participant You\n    participant Agent\n    participant LLM as LanguageModel\n    You-&gt;&gt;Agent: run(\"Hello Agent...\")\n    Agent-&gt;&gt;LLM: Send prompt\n    LLM--&gt;&gt;Agent: Return generated response\n    Agent-&gt;&gt;You: Give final answer</code></pre> <ol> <li>You provide the question or command.  </li> <li>The Agent resolves which model to use and how to structure the prompt.  </li> <li>The language model (LLM) processes the prompt and returns a response.  </li> <li>The Agent sends that final answer back to you.</li> </ol> <p>When Tools and Hooks are involved, the diagram gains extra steps, but this is the core idea.  </p>"},{"location":"tutorial/01_agent_/#5-inside-the-code-a-peek-under-the-hood","title":"5. Inside the Code (A Peek Under the Hood)","text":"<p>The main code for the Agent is in src/redpanda/agents/_agent.py. Key points:</p> <ul> <li>The <code>Agent</code> class has a <code>run(...)</code> method that orchestrates the conversation flow.  </li> <li>If there are Tools, the Agent can detect when the model wants to call them and handle that automatically.  </li> <li><code>AgentHooks</code> offers specialized events to help you track or customize certain moments.  </li> </ul> <p>For example, the Agent checks if the language model\u2019s response indicates a function (tool) call. If so, it calls that tool, captures the result, and continues until it finds a final text answer.</p> <p>Below is an ultra-condensed excerpt (simplified for demonstration) showing how the Agent orchestrates a single response:</p> <pre><code>async def run(self, input_text: str) -&gt; str:\n    # (1) Notify start\n    await self.hooks.on_start(self)\n\n    # (2) Prepare messages and send to LLM\n    messages = [{\"role\": \"user\", \"content\": input_text}]\n    llm_response = await some_llm_completion_call(messages, self.model)\n\n    # (3) If there's a tool call needed, handle it (omitted for brevity)\n    # ...\n\n    # (4) Finalize\n    final_text = llm_response.content\n    await self.hooks.on_end(self, final_text)\n    return final_text\n</code></pre> <p>Explanation of the snippet: - (1) We let the hooks know the Agent is starting. - (2) We build messages for the language model and get a response. - (3) If the response suggests calling a Tool, we handle it (skipped here for review in Tool). - (4) We finalize the text and notify any hooks.  </p>"},{"location":"tutorial/01_agent_/#6-summary-and-next-steps","title":"6. Summary and Next Steps","text":"<p>\u2022 The Agent is your main entry point for interacting with a language model (or multiple Tools) in a clean, modular way. \u2022 It can respond directly or coordinate tasks across different Tools. \u2022 By adjusting properties like instructions, you can shape how the Agent communicates.  </p> <p>In the next chapter, we will introduce the concept of a Tool and see how the Agent cooperates with specialized assistants to solve more complex tasks.  </p> <p>Keep going, and soon you\u2019ll have a flexible, powerful system that can handle a wide range of requests!  </p>"},{"location":"tutorial/02_tool_/","title":"Chapter 2: Tool","text":"<p>In Chapter 1: Agent, we introduced the Agent as our main character who decides how to handle your requests. Sometimes the Agent directly provides answers; but what if the Agent encounters a special task requiring extra skills or data handling? That\u2019s where a Tool comes in.</p>"},{"location":"tutorial/02_tool_/#1-motivation-basic-idea","title":"1. Motivation &amp; Basic Idea","text":"<p>A Tool is like a \u201csidekick\u201d for the Agent. Whenever the Agent needs extra abilities\u2014like looking up data, making calculations, or generating images\u2014it calls on the right Tool with the needed parameters. This is similar to delegating a small job to a trusted co-worker who\u2019s an expert in that specific task.</p> <p>Here\u2019s what makes a Tool so helpful: \u2022 It has a clear name and a brief description describing its specialty. \u2022 It defines the parameters it can accept, like how a function accepts arguments. \u2022 When the Agent can\u2019t do something on its own, it knows which Tool to call and which parameters to send in.</p>"},{"location":"tutorial/02_tool_/#2-the-core-concept-of-a-tool","title":"2. The Core Concept of a Tool","text":"<p>Let\u2019s break down the essentials of a Tool:</p> <ol> <li> <p>Name:    A unique identifier, like \"ImageGenerator\" or \"Calculator\".</p> </li> <li> <p>Description:    A short explanation of what this Tool does.</p> </li> <li> <p>Parameters:    A structured set of arguments (like a schema) the Agent must provide when invoking the Tool.    For example, a Calculator Tool might expect parameters like <code>{ \"operation\": \"add\", \"numbers\": [1,2,3] }</code>.</p> </li> </ol> <p>Once the Agent calls a Tool, the Tool does its specialized job and returns the result. The Agent then incorporates that result into its final answer.</p>"},{"location":"tutorial/02_tool_/#3-a-very-simple-use-case","title":"3. A Very Simple Use Case","text":"<p>Suppose we want our Agent to handle basic math. Rather than programming math logic inside the Agent, we can create a simple Tool called \"NumberCruncher\" that handles calculations. Then, when the Agent sees a user request like \"What is 3 times 4?\" it can call \"NumberCruncher\" to do the math.</p> <p>Below is a minimal code snippet demonstrating how we might define and use a Tool for calculations:</p> <pre><code># number_cruncher_tool.py\nfrom redpanda.agents import Tool\n\nclass NumberCruncher(Tool):\n    \"\"\"\n    A simple math tool that performs arithmetic operations.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"NumberCruncher\",\n            description=\"Performs basic arithmetic operations (add, multiply, etc.).\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"operation\": {\"type\": \"string\"},\n                    \"numbers\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}\n                },\n                \"required\": [\"operation\", \"numbers\"]\n            }\n        )\n\n    async def __call__(self, args: dict) -&gt; float:\n        # 1) Extract parameters\n        operation = args[\"operation\"]\n        numbers = args[\"numbers\"]\n\n        # 2) Perform operation\n        if operation == \"add\":\n            return sum(numbers)\n        elif operation == \"multiply\":\n            product = 1\n            for num in numbers:\n                product *= num\n            return product\n        else:\n            return 0  # fallback or raise an exception\n</code></pre>"},{"location":"tutorial/02_tool_/#explanation-of-the-snippet","title":"Explanation of the Snippet","text":"<p>\u2022 We subclass <code>Tool</code> to create a new type of Tool called <code>NumberCruncher</code>. \u2022 In <code>__init__</code>, we define its name, description, and a JSON schema for parameters (it requires an <code>\"operation\"</code> and an array of <code>\"numbers\"</code>). \u2022 Inside <code>__call__</code>, we implement how the Tool handles requests\u2014here, simple arithmetic.</p>"},{"location":"tutorial/02_tool_/#4-putting-it-all-together-in-the-agent","title":"4. Putting It All Together in the Agent","text":"<p>Imagine we have an Agent that wants to handle math questions. Let\u2019s register our new Tool with the Agent:</p> <pre><code># main.py\nimport asyncio\nfrom redpanda.agents import Agent\nfrom number_cruncher_tool import NumberCruncher\n\n# Create our math tool\nmath_tool = NumberCruncher()\n\n# Create the Agent and attach the tool\nmy_agent = Agent(\n    name=\"MathAgent\",\n    model=\"openai/gpt-3.5-turbo\",\n    instructions=\"You can use NumberCruncher to handle arithmetic.\",\n    tools=[math_tool]\n)\n\n# Agent usage\nasync def run_agent():\n    response = await my_agent.run(\"What is the product of 3 and 4?\")\n    print(response)\n\nasyncio.run(run_agent())\n</code></pre>"},{"location":"tutorial/02_tool_/#explanation-of-what-happens","title":"Explanation of What Happens","text":"<ol> <li>The Agent reads the user\u2019s question: \u201cWhat is the product of 3 and 4?\u201d  </li> <li>It realizes this might require a math operation, so it calls <code>NumberCruncher</code> with something like <code>\"operation\": \"multiply\", \"numbers\": [3,4]</code>.  </li> <li>The Tool returns <code>12</code>.  </li> <li>The Agent includes <code>12</code> in its final answer back to the user.</li> </ol>"},{"location":"tutorial/02_tool_/#5-under-the-hood","title":"5. Under the Hood","text":"<p>Internally, here\u2019s a quick look at how a Tool call happens:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Tool\n    User-&gt;&gt;Agent: \"What is 3 * 4?\"\n    Agent-&gt;&gt;Tool: {operation=\"multiply\", numbers=[3,4]}\n    Tool--&gt;&gt;Agent: 12\n    Agent-&gt;&gt;User: \"The result is 12\"</code></pre> <ol> <li>The Agent identifies the request is arithmetic-related.  </li> <li>The Agent calls the \u201cNumberCruncher\u201d Tool with the right arguments.  </li> <li>The Tool performs the math and returns its result (12).  </li> <li>The Agent includes the result in the final response.</li> </ol>"},{"location":"tutorial/02_tool_/#6-a-quick-peek-at-the-implementation","title":"6. A Quick Peek at the Implementation","text":"<p>In the source code (src/redpanda/agents/_tools.py), the <code>Tool</code> base class looks roughly like this (simplified):</p> <pre><code>class Tool:\n    def __init__(self, name: str, description: str | None, parameters: dict[str, Any]):\n        self.name = name\n        self.description = description\n        self.parameters = parameters\n\n    async def __call__(self, args: dict[str, Any]) -&gt; Any:\n        # Implementation for how the tool is invoked\n        raise NotImplementedError()\n</code></pre> <p>When you subclass <code>Tool</code>, you just override <code>__call__</code> to handle whatever logic you need. The return value can be text, JSON, or a more structured ToolResponse (which we\u2019ll see in the next chapter).</p>"},{"location":"tutorial/02_tool_/#7-summary-and-next-steps","title":"7. Summary and Next Steps","text":"<p>\u2022 A Tool is like a specialized helper function for the Agent. \u2022 You define a name, description, and parameters, then implement the logic in <code>__call__</code>. \u2022 The Agent automatically knows when to call the Tool based on your requests.</p> <p>Next, we\u2019ll look at how the Tool\u2019s output can be packaged in different formats\u2014sometimes simple text, sometimes more complex data structures. Head over to Chapter 3: ToolResponse to learn more!</p>"},{"location":"tutorial/03_toolresponse_/","title":"Chapter 3: ToolResponse","text":"<p>In Chapter 2: Tool, we learned how an Agent can delegate specialized tasks to Tools. But sometimes a Tool needs to return more than simple text\u2014like a text answer plus a base64-encoded image, or multiple pieces of data together. That\u2019s where the concept of a ToolResponse comes in!</p>"},{"location":"tutorial/03_toolresponse_/#1-why-do-we-need-toolresponse","title":"1. Why Do We Need ToolResponse?","text":"<p>Imagine you have an Agent that generates images using a specialized Tool. The Tool might: \u2022 Produce text describing what it did, and \u2022 Also produce the generated image data in base64 form.</p> <p>If the Tool just returns a string, it\u2019s hard to embed all these pieces in a clean, structured way. You\u2019d have to mash text and base64 data together. Confusing, right?</p> <p>ToolResponse acts like a neat \u201cgift basket\u201d for the Tool\u2019s output. It can carry various items\u2014like text content, image content, or more\u2014clearly labeled so the Agent can handle each part gracefully.</p>"},{"location":"tutorial/03_toolresponse_/#2-key-idea-a-container-for-different-output-types","title":"2. Key Idea: A Container for Different Output Types","text":"<p>ToolResponse makes it possible to return multiple \u201ccontents\u201d in one response. Here are two common types of content:</p> <ol> <li>Text Content  </li> <li>Image Content (base64-encoded)</li> </ol> <p>These outputs are stored in a list, allowing Tools to mix and match. An example might look like this: - A text piece giving a summary. - An image piece with a base64-encoded picture.  </p> <p>The Agent can then decide how to present all these pieces to the user.  </p>"},{"location":"tutorial/03_toolresponse_/#3-a-simple-use-case-image-generation-tool","title":"3. A Simple Use Case: Image Generation Tool","text":"<p>Let\u2019s say we have a Tool called <code>PictureMaker</code> that generates an image based on some text prompt. We want to return both a short text saying \u201cHere\u2019s your image!\u201d and the actual image data. Below is a minimal example.</p>"},{"location":"tutorial/03_toolresponse_/#31-defining-a-tool-that-returns-a-toolresponse","title":"3.1 Defining a Tool That Returns a ToolResponse","text":"<pre><code># picture_maker_tool.py\nfrom redpanda.agents import Tool, ToolResponse\nfrom redpanda.agents import ToolResponseTextContent, ToolResponseImageContent\n\nclass PictureMaker(Tool):\n    \"\"\"\n    A simple tool that generates an image and returns both text and the image data.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"PictureMaker\",\n            description=\"Generates an image from a text prompt.\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"prompt\": {\"type\": \"string\"}\n                },\n                \"required\": [\"prompt\"]\n            }\n        )\n\n    async def __call__(self, args: dict) -&gt; ToolResponse:\n        user_prompt = args[\"prompt\"]\n\n        # (1) Imagine we do fancy image generation here.\n        # Skipping the real code for simplicity.\n        generated_image_base64 = \"iVBORw...\"  # pretend base64 data\n\n        # (2) Construct a ToolResponse with text + image\n        return ToolResponse(content=[\n            ToolResponseTextContent(data=\"Here's an image for: \" + user_prompt),\n            ToolResponseImageContent(data=generated_image_base64, mime_type=\"image/png\")\n        ])\n</code></pre>"},{"location":"tutorial/03_toolresponse_/#how-it-works","title":"How It Works","text":"<ol> <li>We subclass <code>Tool</code> and define a JSON schema that requires a <code>\"prompt\"</code> parameter.  </li> <li>Inside <code>__call__</code>, we generate or fetch the image (faked here).  </li> <li>We then build a <code>ToolResponse</code> object with two pieces of content: one text content and one image content.  </li> </ol>"},{"location":"tutorial/03_toolresponse_/#4-using-the-toolresponse-in-the-agent","title":"4. Using the ToolResponse in the Agent","text":"<p>Let\u2019s create an Agent using our <code>PictureMaker</code> tool. Suppose we send the Agent a prompt like \u201cGenerate a small cat image.\u201d The Agent will call <code>PictureMaker</code> behind the scenes and receive a ToolResponse.</p> <pre><code># main.py\nimport asyncio\nfrom redpanda.agents import Agent\nfrom picture_maker_tool import PictureMaker\n\n# 1) Create the tool\npicture_tool = PictureMaker()\n\n# 2) Create the agent and attach the tool\nmy_agent = Agent(\n    name=\"ImageAgent\",\n    model=\"openai/gpt-3.5-turbo\",\n    instructions=\"You can use PictureMaker to generate images from text.\",\n    tools=[picture_tool]\n)\n\n# 3) Run and see what happens\nasync def run_agent():\n    response = await my_agent.run(\"Please generate me a picture of a small cat.\")\n    print(response)  # The Agent will eventually include text + image data in some manner\n\nasyncio.run(run_agent())\n</code></pre>"},{"location":"tutorial/03_toolresponse_/#what-happens-behind-the-scenes","title":"What Happens Behind the Scenes?","text":"<ol> <li>The Agent sees the user prompt: \u201cPlease generate me a picture of a small cat.\u201d  </li> <li>It decides to call the <code>PictureMaker</code> Tool with <code>{ \"prompt\": \"a small cat\" }</code>.  </li> <li>The <code>PictureMaker</code> returns a ToolResponse containing both text (\u201cHere\u2019s an image for: a small cat\u201d) and the base64-encoded image.  </li> <li>The Agent integrates this content into its final output for the user.</li> </ol>"},{"location":"tutorial/03_toolresponse_/#5-step-by-step-internal-flow","title":"5. Step-by-Step Internal Flow","text":"<p>Below is a simplified sequence diagram showing the flow of a request that returns a ToolResponse.</p> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant PictureMaker_Tool as PictureMaker\n    User-&gt;&gt;Agent: \"Picture of cats please\"\n    Agent-&gt;&gt;PictureMaker_Tool: { prompt: \"cats\" }\n    PictureMaker_Tool--&gt;&gt;Agent: ToolResponse (text + image data)\n    Agent-&gt;&gt;User: Final answer with multiple parts</code></pre> <ol> <li>The user requests an image of cats.  </li> <li>The Agent calls the <code>PictureMaker</code> tool with the needed prompt.  </li> <li>The tool returns a <code>ToolResponse</code> including both text and base64 image content.  </li> <li>The Agent compiles these pieces and sends them back in a user-friendly format.</li> </ol>"},{"location":"tutorial/03_toolresponse_/#6-under-the-hood-in-the-code","title":"6. Under the Hood in the Code","text":"<p>In src/redpanda/agents/_tools.py, you\u2019ll see code defining: - <code>ToolResponse</code>, which has a list of content items. - <code>ToolResponseTextContent</code>, representing text. - <code>ToolResponseImageContent</code>, representing a base64-encoded image.</p> <p>Here\u2019s a mini excerpt (simplified):</p> <pre><code># (Simplified) from _tools.py\n\nclass ToolResponseTextContent(BaseModel):\n    type: Literal[\"text\"] = \"text\"\n    data: str\n\nclass ToolResponseImageContent(BaseModel):\n    type: Literal[\"image\"] = \"image\"\n    data: str\n    mime_type: str\n\nclass ToolResponse(BaseModel):\n    content: list[ToolResponseTextContent | ToolResponseImageContent] = []\n</code></pre> <p>When a Tool returns a <code>ToolResponse</code>, the Agent looks at <code>content[]</code> and handles each piece accordingly\u2014combining them into the final user-facing answer.</p>"},{"location":"tutorial/03_toolresponse_/#7-putting-it-all-together","title":"7. Putting It All Together","text":"<p>\u2022 Tools can now return more complex data than plain text. \u2022 <code>ToolResponse</code> bundling helps keep outputs tidy and easy for the Agent to interpret. \u2022 This becomes especially useful when you have text plus attachments (like images) or multiple text snippets.</p>"},{"location":"tutorial/03_toolresponse_/#8-summary-and-next-steps","title":"8. Summary and Next Steps","text":"<p>Congratulations! You\u2019ve just learned how to return structured output from a Tool. This means your Agent can share more nuanced and multimedia-rich responses with the user.</p> <p>In the next chapter, we\u2019ll see how you can tap into special events and customize the Agent\u2019s behavior further by using AgentHooks. Keep learning, and soon you\u2019ll have an even deeper level of insight and control over the Agent\u2019s decision-making process!</p>"},{"location":"tutorial/04_agenthooks_/","title":"Chapter 4: AgentHooks","text":"<p>In the previous chapter: ToolResponse, we saw how a Tool can return complex data (like images + text) back to the Agent. Now, let\u2019s explore a powerful feature called AgentHooks. Think of AgentHooks as a backstage crew helping you monitor or modify an Agent\u2019s behavior at key moments\u2014without cluttering the Agent\u2019s main logic.</p>"},{"location":"tutorial/04_agenthooks_/#1-why-agenthooks","title":"1. Why AgentHooks?","text":"<p>Imagine you want to track how long each Tool call takes or record every prompt the Agent sends. You could embed these features inside the Agent, but that might get messy. AgentHooks solves this by providing special \u201chooks\u201d (callbacks) that run at these pivotal times:</p> <p>\u2022 Before the Agent starts processing. \u2022 After the Agent finishes. \u2022 Right before each Tool is called. \u2022 Right after each Tool finishes.</p> <p>You can use these hooks to log events, gather metrics, or perform custom actions.</p>"},{"location":"tutorial/04_agenthooks_/#2-a-basic-use-case","title":"2. A Basic Use Case","text":"<p>Let\u2019s walk through a simple scenario: we want to measure how much time is spent calling Tools. AgentHooks let us track tool usage without editing the core Agent code.</p> <p>Below is a minimal example of how we can create a custom AgentHooks class that logs tool call durations:</p> <pre><code># custom_hooks.py\nimport time\nfrom redpanda.agents import AgentHooks\n\nclass MyLoggingHooks(AgentHooks):\n    async def on_tool_start(self, agent, tool, args):\n        self.start_time = time.time()\n        print(f\"[HOOK] About to call {tool.name} with args: {args}\")\n\n    async def on_tool_end(self, agent, tool, result):\n        elapsed = time.time() - self.start_time\n        print(f\"[HOOK] {tool.name} finished. Time: {elapsed:.2f} seconds\")\n</code></pre> <p>Explanation: \u2022 We inherit from AgentHooks and override two methods\u2014on_tool_start and on_tool_end. \u2022 When a tool call starts, we store the current time. When it ends, we measure the time elapsed and print it.  </p>"},{"location":"tutorial/04_agenthooks_/#3-attaching-hooks-to-an-agent","title":"3. Attaching Hooks to an Agent","text":"<p>Let\u2019s attach our <code>MyLoggingHooks</code> to an Agent. Now, whenever the Agent decides to use a Tool, our hooks automatically log timing info!</p> <pre><code># main.py\nimport asyncio\nfrom redpanda.agents import Agent\nfrom custom_hooks import MyLoggingHooks\nfrom number_cruncher_tool import NumberCruncher  # from previous chapters\n\nmy_agent = Agent(\n    name=\"AgentWithHooks\",\n    model=\"openai/gpt-3.5-turbo\",\n    tools=[NumberCruncher()],\n    hooks=MyLoggingHooks()\n)\n\n# Run a sample query to see the hooks in action\nasync def run_agent():\n    response = await my_agent.run(\"What is 3 + 4?\")\n    print(\"Final response:\", response)\n\nasyncio.run(run_agent())\n</code></pre> <p>What happens: 1. The Agent sees a math question. 2. It calls <code>NumberCruncher</code>. Our hook logs the start time. 3. The tool returns a result. Our hook logs the end time and prints duration. 4. The Agent gives the final response.</p>"},{"location":"tutorial/04_agenthooks_/#4-step-by-step-under-the-hood","title":"4. Step-by-Step Under the Hood","text":"<p>Here\u2019s a short sequence diagram showing how AgentHooks slip into the flow:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Hooks as MyLoggingHooks\n    participant Tool as NumberCruncher\n    User-&gt;&gt;Agent: \"What is 3 + 4?\"\n    Agent-&gt;&gt;Hooks: on_tool_start(...)\n    Agent-&gt;&gt;Tool: perform calculation\n    Tool--&gt;&gt;Agent: \"7\"\n    Agent-&gt;&gt;Hooks: on_tool_end(...)\n    Agent-&gt;&gt;User: \"7\"</code></pre> <ol> <li>User asks a question.  </li> <li>The Agent notifies our hook before calling the tool.  </li> <li>The tool does its work.  </li> <li>The Agent notifies our hook after the tool finishes.  </li> <li>The Agent returns the final answer.</li> </ol>"},{"location":"tutorial/04_agenthooks_/#5-peeking-at-the-internal-implementation","title":"5. Peeking at the Internal Implementation","text":"<p>Internally, the Agent\u2019s code (in src/redpanda/agents/_agent.py) calls these callbacks at specific times:</p> <ol> <li>on_start(...) before the Agent begins (great for resetting logs or counters).  </li> <li>on_tool_start(...) when a Tool is about to run.  </li> <li>on_tool_end(...) right after a Tool returns.  </li> <li>on_end(...) once the Agent has a final answer.</li> </ol> <p>A simplified snippet looks something like this (shortened for clarity):</p> <pre><code># (Simplified) from _agent.py\nclass Agent:\n    async def run(self, input: str) -&gt; str:\n        # 1) Notify that the Agent is starting\n        await self.hooks.on_start(self)\n\n        # 2) ...\n        # If a tool is called:\n        await self.hooks.on_tool_start(self, some_tool, \"args...\")\n        result = await some_tool.invoke(...)\n        await self.hooks.on_tool_end(self, some_tool, result)\n\n        # 3) Finally, notify that the Agent ended\n        await self.hooks.on_end(self, \"final output\")\n        return \"final output\"\n</code></pre> <p>By plugging in your own subclass, you can decide what to do at each step without editing the Agent\u2019s core logic.</p>"},{"location":"tutorial/04_agenthooks_/#6-summary-next-steps","title":"6. Summary &amp; Next Steps","text":"<p>\u2022 AgentHooks lets you watch and react to important moments in the Agent\u2019s lifecycle\u2014like a backstage manager that sees everything. \u2022 We created a custom hook to measure tool call durations. You could expand this idea to log prompts, capture stats, or alter behavior. \u2022 With these hooks, you have a clean way to add \u201cside features\u201d while keeping your Agent code neat.</p> <p>Ready to explore more advanced capabilities? In the next chapter: MCPEndpoint, we\u2019ll see how to connect Agents with external servers for more powerful capabilities!</p>"},{"location":"tutorial/05_mcpendpoint_/","title":"Chapter 5: MCPEndpoint","text":"<p>In the previous chapter: AgentHooks, we saw how to track key moments in an Agent\u2019s workflow. Now, let\u2019s shift our focus to something more \u201coutward facing\u201d: connecting your Agent to external servers! This is where the class \u201cMCPEndpoint\u201d comes in.</p>"},{"location":"tutorial/05_mcpendpoint_/#1-why-mcpendpoint","title":"1. Why MCPEndpoint?","text":"<p>Sometimes, your Agent might not have all the tools it needs locally. Or maybe you want to delegate tasks to a remote service. MCPEndpoint is like creating a \u201cphone line\u201d for your Agent to talk to any external MCP (Model Context Protocol) server, whether it\u2019s running on your local machine or out on the internet.</p> <p>For example: \u2022 You can connect via standard input/output (like launching a program and talking with it). \u2022 You can connect via Server-Sent Events (SSE). \u2022 You can connect via WebSocket.  </p> <p>Think of MCPEndpoint as the base blueprint. Different subclasses (e.g., StdioMCPEndpoint, SSEMCPEndpoint, WebsocketMCPEndpoint) each handle one style of communication.</p>"},{"location":"tutorial/05_mcpendpoint_/#2-simple-use-case-connecting-to-a-remote-service","title":"2. Simple Use Case: Connecting to a Remote Service","text":"<p>Let\u2019s say you have a remote MCP server at \u201chttps://my-remote-service.com/sse.\u201d You want your Agent to call Tools that live on that server. Step one is to create an SSEMCPEndpoint and then load it into an MCP client.</p> <p>Below is a minimal code snippet that shows how to do this in under 20 lines:</p> <pre><code># main.py\nimport asyncio\nfrom redpanda.agents import SSEMCPEndpoint\nfrom redpanda.agents._mcp import mcp_client\n\nasync def connect_to_remote():\n    endpoint = SSEMCPEndpoint(url=\"https://my-remote-service.com/sse\")\n\n    # Create a client session\n    async with mcp_client(endpoint) as client:\n        # Initialize communication\n        await client.initialize()\n\n        # Fetch a list of Tools from the remote MCP server\n        tools = await client.list_tools()\n        print(\"Tools found:\", [t.name for t in tools])\n\nasyncio.run(connect_to_remote())\n</code></pre>"},{"location":"tutorial/05_mcpendpoint_/#explanation","title":"Explanation","text":"<ol> <li>We create an SSEMCPEndpoint pointing to our remote SSE URL.  </li> <li>We call \u201cmcp_client(endpoint)\u201d to open a session with that remote server.  </li> <li>We call \u201cclient.initialize()\u201d to get everything ready.  </li> <li>We fetch a list of Tools that the server says it can handle.  </li> <li>We print them just to see what\u2019s available.</li> </ol>"},{"location":"tutorial/05_mcpendpoint_/#3-mcpendpoint-in-action","title":"3. MCPEndpoint in Action","text":"<p>Once your endpoint is created, the Agent can seamlessly use the remote Tools. For instance, if your MCP server exposes a math or image-generation tool, the Agent can discover and call it\u2014just like a local Tool. This means your system can \u201creach out\u201d to specialized services whenever needed.</p> <p>Below is a simplified sequence of what happens when your Agent calls a remote tool through MCPEndpoint:</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant Endpoint as MCPEndpoint\n    participant Client as MCPClient\n    participant Server as MCP Server\n    Agent-&gt;&gt;Endpoint: \"I need to talk to remote tools\"\n    Endpoint-&gt;&gt;Client: Build session (mcp_client)\n    Client-&gt;&gt;Server: \"list_tools\" or \"call_tool\"\n    Server--&gt;&gt;Client: Available tools or result\n    Client--&gt;&gt;Endpoint: Final data\n    Endpoint--&gt;&gt;Agent: \"Here are the tool results!\"</code></pre> <ol> <li>The Agent has an Endpoint describing how to talk to the server.  </li> <li>That Endpoint creates a Client session to manage the actual read/write communication.  </li> <li>The Client calls the server\u2019s API (like \u201clist_tools\u201d or \u201ccall_tool\u201d).  </li> <li>The server replies with tool definitions or tool outputs.  </li> <li>The Endpoint returns the result back to the Agent.</li> </ol>"},{"location":"tutorial/05_mcpendpoint_/#4-under-the-hood","title":"4. Under the Hood","text":"<p>Behind the scenes, MCPEndpoint is just an agreement for how to connect. Let\u2019s see a tiny peek inside the file src/redpanda/agents/_mcp.py. Don\u2019t worry if it looks advanced\u2014this is just to show how it fits together.</p> <pre><code>class MCPEndpoint:\n    def __init__(self, cache_enabled: bool):\n        self._cache_enabled = cache_enabled\n        self._cached_tool_list = None\n\nclass SSEMCPEndpoint(MCPEndpoint):\n    def __init__(self, url: str, cache_enabled: bool = True):\n        super().__init__(cache_enabled)\n        self.url = url\n</code></pre> <p>\u2022 MCPEndpoint stores basic settings (like whether to cache the list of tools). \u2022 SSEMCPEndpoint extends it, adding a \u201curl\u201d for connecting via SSE.</p> <p>Later, there\u2019s a helper function, \u201cmcp_client(...),\u201d that uses the right client logic (SSE, stdin/stdout, WebSocket, etc.) based on which MCPEndpoint you gave it.</p>"},{"location":"tutorial/05_mcpendpoint_/#5-example-calling-a-remote-tool","title":"5. Example: Calling a Remote Tool","text":"<p>Once you\u2019ve created and initialized your MCPClient, you can call any Tool the server provides. Below is a small example showing how to call a single remote Tool by name:</p> <pre><code># call_remote_tool.py\nimport asyncio\nfrom redpanda.agents import SSEMCPEndpoint\nfrom redpanda.agents._mcp import mcp_client\n\nasync def invoke_math_tool():\n    endpoint = SSEMCPEndpoint(url=\"https://my-remote-service.com/sse\")\n    async with mcp_client(endpoint) as client:\n        await client.initialize()\n\n        # Suppose there's a \"RemoteMath\" tool on the server\n        result = await client.call_tool(\"RemoteMath\", {\"operation\": \"add\", \"numbers\": [2, 5]})\n        print(\"Result from remote math tool:\", result)\n\nasyncio.run(invoke_math_tool())\n</code></pre> <p>Explanation: \u2022 We connect to the same SSE endpoint. \u2022 We call \u201cinvoke_math_tool\u201d with the necessary parameters. \u2022 The server does the math and returns the result (maybe a ToolResponse object) back to us.</p>"},{"location":"tutorial/05_mcpendpoint_/#6-when-to-use-mcpendpoint-vs-local-tools","title":"6. When to Use MCPEndpoint vs Local Tools","text":"<p>\u2022 Use MCPEndpoint if you want to leverage Tools that don\u2019t live inside your Python code\u2014like an external microservice or a specialized engine. \u2022 If your tasks are self-contained (like a small math function), you can keep the Tool local. \u2022 The beauty of MCPEndpoint is that your Agent can automatically discover and interact with new Tools as they\u2019re added to the remote server\u2014no need to update your local code!</p>"},{"location":"tutorial/05_mcpendpoint_/#7-summary-next-steps","title":"7. Summary &amp; Next Steps","text":"<p>\u2022 MCPEndpoint is your \u201cbridge\u201d to external servers hosting Tools. \u2022 Different subclasses handle different connection methods: Stdio, SSE, WebSocket, etc. \u2022 You gain the flexibility of calling local or remote Tools in a uniform way.</p> <p>Next time, we\u2019ll look more closely at mcp_client, which is the library helper that actually does most of the handshake and messaging between your Python code and the server. You\u2019ll see how it all fits together behind the scenes!</p>"},{"location":"tutorial/07_mcp_client_/","title":"Chapter 6: mcp_client","text":"<p>In the previous chapter: MCPEndpoint, you learned how an Agent can communicate with external servers or local processes using different connection methods. Now, let's explore the core function that actually sets up the conversation with any MCP server\u2014whether it\u2019s local, remote, using Standard I/O, SSE, or WebSocket:</p> <p>\u2022 The function name: <code>mcp_client</code> \u2022 Purpose: Creates a temporary communication channel to an MCP server. \u2022 Think of it like starting a quick \u201cconference call,\u201d doing some tasks (like listing available Tools or calling them), and then hanging up.</p>"},{"location":"tutorial/07_mcp_client_/#1-why-we-need-mcp_client","title":"1. Why We Need mcp_client","text":"<p>Imagine your Agent wants to explore new Tools hosted in a remote server. You don\u2019t want your Agent to manually open a connection, authenticate, request tool data, parse responses, etc. That\u2019s a lot of repetitive work.</p> <p>The <code>mcp_client</code> function does all of that heavy lifting: 1. It takes an MCPEndpoint (which specifies how and where to connect). 2. It sets up the communication wires. 3. Inside a friendly Python <code>async with</code> block, you can list or call Tools. 4. When the block ends, it cleans up everything automatically (like hanging up a call).</p>"},{"location":"tutorial/07_mcp_client_/#2-a-simple-use-case","title":"2. A Simple Use Case","text":"<p>Let\u2019s say we have an endpoint to a local process or to a remote server. We just want to: 1. Start the client session. 2. Fetch the available Tools. 3. Print the names of those Tools.  </p> <p>Below is a super short code snippet (fewer than 20 lines) showing how to do it:</p> <pre><code>import asyncio\nfrom redpanda.agents._mcp import mcp_client\n# Suppose 'my_endpoint' is an MCPEndpoint from a previous example.\n\nasync def list_remote_tools(my_endpoint):\n    async with mcp_client(my_endpoint) as client:\n        await client.initialize()      # Start the conversation\n        tools = await client.list_tools()\n        print(\"Available Tools:\", [t.name for t in tools])\n\n# Usage\n# asyncio.run(list_remote_tools(my_endpoint))\n</code></pre>"},{"location":"tutorial/07_mcp_client_/#how-it-works","title":"How It Works","text":"<ol> <li>We enter the <code>async with mcp_client(my_endpoint)</code> block, which dials into the MCP process.  </li> <li>We call <code>client.initialize()</code> to let the server know we\u2019re ready.  </li> <li>We request the list of Tools and print their names.  </li> <li>After we exit the <code>async with</code> block, the line is automatically closed.</li> </ol>"},{"location":"tutorial/07_mcp_client_/#3-step-by-step-under-the-hood","title":"3. Step-by-Step Under the Hood","text":"<p>Below is a quick, simplified sequence diagram showing the main participants in a typical scenario where we list Tools over an MCP connection:</p> <pre><code>sequenceDiagram\n    participant YourCode\n    participant MCPClient as mcp_client(...)\n    participant Server as MCP Server\n    YourCode-&gt;&gt;MCPClient: \"Open session\"\n    MCPClient-&gt;&gt;Server: \"Hello, let's talk\"\n    YourCode-&gt;&gt;MCPClient: list_tools()\n    MCPClient-&gt;&gt;Server: \"list_tools\"\n    Server--&gt;&gt;MCPClient: Tool definitions\n    MCPClient--&gt;&gt;YourCode: \"Here are the tools\"\n    YourCode-&gt;&gt;MCPClient: \"Done\"\n    MCPClient--&gt;&gt;Server: \"Goodbye\"</code></pre> <ol> <li>Your code calls <code>mcp_client(...)</code> with an Endpoint.  </li> <li><code>mcp_client</code> sets up the channel to the server.  </li> <li>You run your desired commands (like <code>list_tools()</code>).  </li> <li>Once finished, the session ends politely.</li> </ol>"},{"location":"tutorial/07_mcp_client_/#4-calling-a-single-tool","title":"4. Calling a Single Tool","text":"<p>If you already know the name of a Tool and want to call it, that\u2019s easy too. Here\u2019s a small snippet that\u2019s under 20 lines:</p> <pre><code>import asyncio\nfrom redpanda.agents._mcp import mcp_client\n\nasync def call_math_tool(endpoint):\n    async with mcp_client(endpoint) as client:\n        await client.initialize()\n        result = await client.call_tool(\"AdderTool\", {\"numbers\": [1, 2, 3]})\n        print(\"Sum result:\", result)\n\n# Usage\n# asyncio.run(call_math_tool(my_endpoint))\n</code></pre> <p>Explanation: \u2022 We open the session and initialize. \u2022 We call a tool named \u201cAdderTool\u201d with <code>{\"numbers\": [1,2,3]}</code>. \u2022 The server does the math, and we print the result.</p>"},{"location":"tutorial/07_mcp_client_/#5-a-peek-at-the-internal-implementation","title":"5. A Peek at the Internal Implementation","text":"<p>Here\u2019s a shortened version of the <code>mcp_client</code> function as found in src/redpanda/agents/_mcp.py. Notice how it handles different Endpoint types (like standard I/O, SSE, or WebSocket) with the same base protocol:</p> <pre><code>@asynccontextmanager\nasync def mcp_client(server: MCPEndpoint):\n    if isinstance(server, StdioMCPEndpoint):\n        async with stdio_client(server.params) as (read, write):\n            async with ClientSession(read, write) as session:\n                yield MCPClient(server, session)\n    elif isinstance(server, SSEMCPEndpoint):\n        async with sse_client(server.url) as (read, write):\n            async with ClientSession(read, write) as session:\n                yield MCPClient(server, session)\n    # ... similarly for WebSocket\n</code></pre>"},{"location":"tutorial/07_mcp_client_/#whats-happening","title":"What\u2019s Happening?","text":"<ol> <li>Based on the <code>MCPEndpoint</code> type, it sets up the correct input/output channels (stdio, SSE, or WebSocket).  </li> <li>It creates a <code>ClientSession</code> (lower-level object to handle protocol details).  </li> <li>Finally, it yields an <code>MCPClient</code>, which has higher-level functions like <code>list_tools()</code> and <code>call_tool()</code>.</li> </ol>"},{"location":"tutorial/07_mcp_client_/#6-summary-and-next-steps","title":"6. Summary and Next Steps","text":"<p>Through <code>mcp_client</code>, your Agent can conveniently \u201cdial\u201d into any MCP server, gather available Tools, and call them as needed\u2014then hang up. This function\u2019s design keeps everything neat and consistent, no matter which transport method you choose.</p> <p>\u2022 We learned why <code>mcp_client</code> is essential\u2014managing the connection, initialization, and teardown with minimal effort. \u2022 We saw how to list Tools or call them directly. \u2022 We peeked behind the scenes at how it picks the right communication method.</p> <p>In the next chapter, RuntimeServer, we\u2019ll explore a bigger picture: how to host and run these Tools as a server process, so others can connect via <code>mcp_client</code> to your own system. Stay tuned!</p>"},{"location":"tutorial/08_runtimeserver_/","title":"Chapter 7: RuntimeServer","text":"<p>In Chapter 6: mcp_client, we learned how to tap into an existing server that hosts tools or services. But what if you want others to tap into your Agent in the same way\u2014via a simple, well-defined interface? That\u2019s exactly where \u201cRuntimeServer\u201d shines.</p> <p>Imagine you have an Agent with all sorts of logic or tooling behind it, and you want to let an external system send requests. The external system might not speak Python, or it might be on a totally different tech stack. By using RuntimeServer, you can expose your Agent as a gRPC service\u2014letting outside callers invoke it in real time, just like a translator who relays messages back and forth.</p>"},{"location":"tutorial/08_runtimeserver_/#1-why-runtimeserver","title":"1. Why RuntimeServer?","text":"<p>The primary goal: make your Agent accessible to external systems via gRPC calls. Here\u2019s the high-level idea: \u2022 External callers send structured requests (e.g., Protobuf or JSON data). \u2022 RuntimeServer receives those gRPC calls and translates them into a format the Agent understands. \u2022 The Agent processes the request (possibly calling Tools) and returns a response. \u2022 RuntimeServer then wraps the response back into gRPC format for the caller.</p> <p>This flow means you can integrate your Python Agent into larger applications and microservices\u2014enabling real-time processing without forcing everyone else to speak Python.</p>"},{"location":"tutorial/08_runtimeserver_/#2-a-very-simple-example","title":"2. A Very Simple Example","text":"<p>Below is a minimal snippet demonstrating how to start a RuntimeServer around an Agent. This lets external gRPC clients call \u201cInvokeAgent\u201d operations:</p> <pre><code>import asyncio\nfrom redpanda.agents import Agent\nfrom redpanda.runtime._grpc import RuntimeServer, serve_main\n\nasync def main():\n    # 1) Create an Agent with minimal setup\n    my_agent = Agent(\n        name=\"MyRuntimeAgent\",\n        model=\"openai/gpt-3.5-turbo\",\n        instructions=\"You are now accessible via gRPC. Respond clearly.\"\n    )\n\n    # 2) Create a RuntimeServer with that Agent\n    server = RuntimeServer(my_agent)\n\n    # 3) Start the server (it will pick a port automatically and print it)\n    await serve_main(server)\n\nasyncio.run(main())\n</code></pre> <p>Explanation: 1. We import our Agent and the server utilities. 2. We create an Agent exactly as in previous chapters (optionally including Tools, instructions, etc.). 3. We wrap that Agent in <code>RuntimeServer</code> and call <code>serve_main(...)</code>. 4. Once this is running, external gRPC clients can connect, send a message, and get a response from the Agent.</p>"},{"location":"tutorial/08_runtimeserver_/#3-step-by-step-flow-under-the-hood","title":"3. Step-by-Step Flow Under the Hood","text":"<p>When an external system calls your new gRPC service, the interaction looks like this:</p> <pre><code>sequenceDiagram\n    participant ExternalSystem\n    participant RuntimeServer\n    participant Agent\n    participant LLM as LanguageModel\n\n    ExternalSystem-&gt;&gt;RuntimeServer: gRPC: InvokeAgent(\"Hello from outside!\")\n    RuntimeServer-&gt;&gt;Agent: run(\"Hello from outside!\")\n    Agent-&gt;&gt;LLM: Access language model or Tools\n    LLM--&gt;&gt;Agent: \"Hi external user!\"\n    Agent--&gt;&gt;RuntimeServer: \"Hi external user!\"\n    RuntimeServer--&gt;&gt;ExternalSystem: gRPC: \"Hi external user!\"</code></pre> <ol> <li>The external system sends data via gRPC.  </li> <li>RuntimeServer translates that data into plain text or JSON for the Agent.  </li> <li>The Agent uses its logic or even calls Tools (if needed).  </li> <li>The Agent returns the final response to RuntimeServer.  </li> <li>RuntimeServer wraps it back into a gRPC message and sends it to the external caller.</li> </ol>"},{"location":"tutorial/08_runtimeserver_/#4-handling-structured-data","title":"4. Handling Structured Data","text":"<p>In many cases, external clients prefer sending structured data (like JSON or base64-encoded images). RuntimeServer automatically handles these conversions: \u2022 If the caller sends a structured Protobuf payload, RuntimeServer can turn that into JSON or text for the Agent. \u2022 If the Agent returns complex data, RuntimeServer can package it back into Protobuf (e.g., base64 for binary content).  </p> <p>This all happens behind the scenes, so your Agent can focus on logic instead of worrying about how to parse or serialize messages.</p>"},{"location":"tutorial/08_runtimeserver_/#5-where-the-runtimeserver-code-lives","title":"5. Where the RuntimeServer Code Lives","text":"<p>The core code for RuntimeServer can be found in src/redpanda/runtime/_grpc.py. If you open it, you\u2019ll see something like this (shortened and simplified):</p> <pre><code># (Simplified) runtime/_grpc.py\n\nclass RuntimeServer(grpcpb.RuntimeServicer):\n    def __init__(self, agent: Agent):\n        self.agent = agent\n\n    async def InvokeAgent(self, request, context):\n        try:\n            # Convert incoming message to what the Agent understands\n            payload = request.message.serialized.decode(\"utf-8\")\n            output = await self.agent.run(payload)\n\n            # Wrap the Agent's response in gRPC form\n            return pb.InvokeAgentResponse(\n                message=pb.Message(\n                    serialized=output.encode(\"utf-8\")\n                ),\n            )\n        except Exception as e:\n            context.set_code(grpc.StatusCode.INTERNAL)\n            context.set_details(str(e))\n            raise\n</code></pre> <p>Explanation of the snippet: 1. <code>RuntimeServer</code> inherits from <code>RuntimeServicer</code>, a gRPC class stub generated from .proto definitions. 2. In <code>InvokeAgent</code>, it reads the request payload and decodes it to text (or structured JSON if needed). 3. It then calls <code>agent.run(...)</code> passing that text. 4. Whatever text or JSON the Agent returns is wrapped back into a gRPC <code>InvokeAgentResponse</code> message.</p> <p>The <code>serve_main(server)</code> function further sets up a gRPC server object, binds to a port, and listens for requests.</p>"},{"location":"tutorial/08_runtimeserver_/#6-ready-to-let-others-in","title":"6. Ready to Let Others In","text":"<p>With the RuntimeServer approach, any application that can make gRPC calls can now ask your Agent questions. For example: \u2022 A Node.js or Java service can say, \u201cHey, Agent, how do I parse this data?\u201d \u2022 A web front-end can call your service through a gateway. \u2022 Another microservice performing data transformation can query your Agent for more advanced reasoning.</p> <p>In all of these scenarios, your Python Agent is no longer locked inside local script-land. It\u2019s out in the open, ready to serve real-time requests.</p>"},{"location":"tutorial/08_runtimeserver_/#7-conclusion","title":"7. Conclusion","text":"<p>\u2022 RuntimeServer is the final piece that lets you expose your Agent to the outside world through gRPC. \u2022 It automatically manages data translation to and from Protobuf, so your Agent can focus on actual reasoning. \u2022 By combining Agent, Tools, AgentHooks, and now RuntimeServer, you have a complete pipeline for building intelligent, extensible applications.</p> <p>This concludes our entire journey! You now have the know-how to build Agents, create Tools, gather responses, hook into key events, connect to external resources, and serve your Agent via gRPC. Happy building, and thank you for following along!</p>"}]}